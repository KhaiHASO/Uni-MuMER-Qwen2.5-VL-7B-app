"""
Module Ä‘á»ƒ load vÃ  sá»­ dá»¥ng mÃ´ hÃ¬nh Uni-MuMER-Qwen2.5-VL-7B
"""

import torch
from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor
import logging
from typing import Optional, Tuple, Union
from PIL import Image
import os

# Cáº¥u hÃ¬nh logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class UniMuMERModel:
    """Class Ä‘á»ƒ quáº£n lÃ½ mÃ´ hÃ¬nh Uni-MuMER-Qwen2.5-VL-7B"""
    
    def __init__(self, model_path: str = "./Uni-MuMER-Qwen2.5-VL-7B"):
        """
        Khá»Ÿi táº¡o mÃ´ hÃ¬nh
        
        Args:
            model_path (str): ÄÆ°á»ng dáº«n Ä‘áº¿n thÆ° má»¥c chá»©a mÃ´ hÃ¬nh
        """
        self.model_path = model_path
        self.model = None
        self.processor = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
    def load_model(self) -> bool:
        """
        Load mÃ´ hÃ¬nh vÃ  cÃ¡c components
        
        Returns:
            bool: True náº¿u load thÃ nh cÃ´ng, False náº¿u cÃ³ lá»—i
        """
        try:
            logger.info(f"Äang load mÃ´ hÃ¬nh tá»«: {self.model_path}")
            
            # Kiá»ƒm tra Ä‘Æ°á»ng dáº«n mÃ´ hÃ¬nh
            if not os.path.exists(self.model_path):
                logger.error(f"KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c mÃ´ hÃ¬nh: {self.model_path}")
                return False
            
            # Load processor vÃ  tokenizer
            logger.info("Äang load processor vÃ  tokenizer...")
            self.processor = AutoProcessor.from_pretrained(
                self.model_path, 
                trust_remote_code=True
            )
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path, 
                trust_remote_code=True
            )
            
            # Load mÃ´ hÃ¬nh
            logger.info("Äang load mÃ´ hÃ¬nh...")
            self.model = Qwen2VLForConditionalGeneration.from_pretrained(
                self.model_path,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                trust_remote_code=True
            )
            
            logger.info("Load mÃ´ hÃ¬nh thÃ nh cÃ´ng!")
            return True
            
        except Exception as e:
            logger.error(f"Lá»—i khi load mÃ´ hÃ¬nh: {str(e)}")
            return False
    
    def is_loaded(self) -> bool:
        """Kiá»ƒm tra xem mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c load chÆ°a"""
        return self.model is not None and self.processor is not None and self.tokenizer is not None
    
    def convert_image_to_latex(
        self, 
        image: Union[Image.Image, str], 
        prompt: str = None,
        max_new_tokens: int = 512,
        temperature: float = 0.1,
        top_p: float = 0.9,
        repetition_penalty: float = 1.1
    ) -> Optional[str]:
        """
        Chuyá»ƒn Ä‘á»•i áº£nh toÃ¡n há»c thÃ nh LaTeX
        
        Args:
            image: áº¢nh Ä‘áº§u vÃ o (PIL Image hoáº·c Ä‘Æ°á»ng dáº«n file)
            prompt: Prompt Ä‘á»ƒ hÆ°á»›ng dáº«n mÃ´ hÃ¬nh
            max_new_tokens: Sá»‘ token tá»‘i Ä‘a Ä‘á»ƒ generate
            temperature: Nhiá»‡t Ä‘á»™ sampling
            top_p: Top-p sampling
            repetition_penalty: Penalty cho repetition
            
        Returns:
            str: LaTeX code hoáº·c None náº¿u cÃ³ lá»—i
        """
        if not self.is_loaded():
            logger.error("MÃ´ hÃ¬nh chÆ°a Ä‘Æ°á»£c load!")
            return None
        
        try:
            # Load áº£nh náº¿u lÃ  Ä‘Æ°á»ng dáº«n
            if isinstance(image, str):
                image = Image.open(image)
            
            # Prompt máº·c Ä‘á»‹nh
            if prompt is None:
                prompt = "Convert this mathematical image to LaTeX format. Only output the LaTeX code without any explanation."
            
            # Chuáº©n bá»‹ messages
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": image},
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
            
            # Tokenize
            text = self.processor.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            image_inputs, video_inputs = self.processor.process_vision_info(messages)
            inputs = self.processor(
                text=[text], 
                images=image_inputs, 
                videos=video_inputs, 
                return_tensors="pt"
            )
            
            # Generate
            logger.info("Äang generate LaTeX...")
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=False,
                    temperature=temperature,
                    top_p=top_p,
                    repetition_penalty=repetition_penalty
                )
            
            # Decode
            generated_ids = [
                output_ids[len(input_ids):] 
                for input_ids, output_ids in zip(inputs.input_ids, generated_ids)
            ]
            response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
            
            # LÃ m sáº¡ch output
            latex_code = self._clean_latex_output(response)
            
            logger.info("Generate LaTeX thÃ nh cÃ´ng!")
            return latex_code
            
        except Exception as e:
            logger.error(f"Lá»—i khi chuyá»ƒn Ä‘á»•i áº£nh: {str(e)}")
            return None
    
    def _clean_latex_output(self, text: str) -> str:
        """
        LÃ m sáº¡ch output Ä‘á»ƒ chá»‰ láº¥y LaTeX code
        
        Args:
            text: Text output tá»« mÃ´ hÃ¬nh
            
        Returns:
            str: LaTeX code Ä‘Ã£ lÃ m sáº¡ch
        """
        import re
        
        # Loáº¡i bá» cÃ¡c tá»« khÃ³a khÃ´ng cáº§n thiáº¿t
        text = re.sub(r'^(LaTeX|latex|Latex):?\s*', '', text, flags=re.IGNORECASE)
        text = re.sub(r'^(Here is|The|This is).*?:\s*', '', text, flags=re.IGNORECASE)
        
        # TÃ¬m vÃ  trÃ­ch xuáº¥t LaTeX code trong dáº¥u $$
        latex_match = re.search(r'\$\$(.*?)\$\$', text, re.DOTALL)
        if latex_match:
            return latex_match.group(1).strip()
        
        # TÃ¬m LaTeX code trong dáº¥u $
        latex_match = re.search(r'\$(.*?)\$', text, re.DOTALL)
        if latex_match:
            return latex_match.group(1).strip()
        
        # Náº¿u khÃ´ng tÃ¬m tháº¥y dáº¥u $, tráº£ vá» text Ä‘Ã£ lÃ m sáº¡ch
        return text.strip()
    
    def get_model_info(self) -> dict:
        """
        Láº¥y thÃ´ng tin vá» mÃ´ hÃ¬nh
        
        Returns:
            dict: ThÃ´ng tin mÃ´ hÃ¬nh
        """
        info = {
            "model_path": self.model_path,
            "device": self.device,
            "is_loaded": self.is_loaded(),
            "cuda_available": torch.cuda.is_available()
        }
        
        if self.is_loaded():
            info.update({
                "model_type": type(self.model).__name__,
                "tokenizer_type": type(self.tokenizer).__name__,
                "processor_type": type(self.processor).__name__
            })
        
        return info

# HÃ m tiá»‡n Ã­ch Ä‘á»ƒ sá»­ dá»¥ng nhanh
def quick_convert(image_path: str, model_path: str = "./Uni-MuMER-Qwen2.5-VL-7B") -> Optional[str]:
    """
    HÃ m tiá»‡n Ã­ch Ä‘á»ƒ chuyá»ƒn Ä‘á»•i nhanh áº£nh thÃ nh LaTeX
    
    Args:
        image_path: ÄÆ°á»ng dáº«n Ä‘áº¿n file áº£nh
        model_path: ÄÆ°á»ng dáº«n Ä‘áº¿n thÆ° má»¥c mÃ´ hÃ¬nh
        
    Returns:
        str: LaTeX code hoáº·c None náº¿u cÃ³ lá»—i
    """
    model = UniMuMERModel(model_path)
    
    if not model.load_model():
        return None
    
    return model.convert_image_to_latex(image_path)

if __name__ == "__main__":
    # Test mÃ´ hÃ¬nh
    model = UniMuMERModel()
    
    if model.load_model():
        print("âœ… Load mÃ´ hÃ¬nh thÃ nh cÃ´ng!")
        print(f"ğŸ“Š ThÃ´ng tin mÃ´ hÃ¬nh: {model.get_model_info()}")
    else:
        print("âŒ KhÃ´ng thá»ƒ load mÃ´ hÃ¬nh!")
